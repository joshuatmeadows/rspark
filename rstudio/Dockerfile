FROM rocker/verse:3.3.3
ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64


ARG pgversion=9.4
ARG hadoopversion=2.7.1
ARG sparkversion=2.2.0
ARG hiveversion=2.1.1



# Update machine and install 
RUN apt-get update && apt-get install -y postgresql-client-${pgversion} libicu-dev && apt-get clean

####################
# JAVA
####################
RUN echo "deb http://http.debian.net/debian jessie-backports main" > /etc/apt/sources.list.d/jessie-backports.list
RUN apt-get update && apt-get install -y -t jessie-backports openjdk-8-jdk
# config Java within R for rJava installation
RUN sudo R CMD javareconf
# Install Google Protocol Buffer
# ADD protobuf-2.5.0.tar.gz /tmp
# RUN cd /tmp/protobuf-2.5.0 && ./configure && make -j4 && make install && cd .. && rm -rf protobuf-*

####################
# HADOOP
####################
# Download and Install Hadoop and set Hadoop environment variable
RUN cd /opt && wget --quiet http://archive.apache.org/dist/hadoop/core/hadoop-${hadoopversion}/hadoop-${hadoopversion}.tar.gz && \
	tar zxf hadoop-${hadoopversion}.tar.gz && \
	ln -s hadoop-${hadoopversion} hadoop && \
	rm hadoop-${hadoopversion}.tar.gz && \
	(cd /opt/hadoop; ln -s share/hadoop/tools/lib/hadoop-streaming-${hadoopversion}.jar hadoop-streaming.jar)
RUN mkdir native && cd native && \
	wget --quiet https://github.com/sequenceiq/docker-hadoop-build/releases/download/v2.7.1/hadoop-native-64-2.7.1.tgz && \
	tar zxf hadoop-native-64-2.7.1.tgz && \
	rm hadoop-native-64-2.7.1.tgz && \
	cd ..
RUN rm -rf /opt/hadoop/lib/native && mv native /opt/hadoop/lib && chown -R rstudio:rstudio /opt/hadoop
ENV HADOOP_CMD /opt/hadoop/bin/hadoop

####################
# SPARK
####################
# Install Spark
RUN cd /opt && \
	wget --quiet http://apache.mirrors.ionfish.org/spark/spark-${sparkversion}/spark-${sparkversion}-bin-hadoop2.7.tgz && \
	tar zxf spark-${sparkversion}-bin-hadoop2.7.tgz && \
	mv spark-${sparkversion}-bin-hadoop2.7 spark && \
	cp spark/conf/spark-env.sh.template spark/conf/spark-env.sh && \
	echo "export SPARK_DIST_CLASSPATH=$(/opt/hadoop/bin/hadoop classpath)" >> spark/conf/spark-env.sh


####################
# HIVE
####################
RUN cd /opt && \
	wget --quiet http://apache.mirrors.ionfish.org/hive/hive-${hiveversion}/apache-hive-${hiveversion}-bin.tar.gz && \
	tar zxf apache-hive-${hiveversion}-bin.tar.gz && \
	ln -s apache-hive-${hiveversion}-bin hive && \
	ln -s /opt/hive/jdbc/hive-jdbc-${hiveversion}-standalone.jar /opt/hive/lib/ && \
	rm apache-hive-${hiveversion}-bin.tar.gz

####################
# R PACKAGES
####################
# Switch to rstudio CRAN mirror (untested)
# RUN R CMD options(repos = c(CRAN = "https://cran.rstudio.com"))
# Set environment variable for rJava package installation
ENV LD_LIBRARY_PATH $JAVA_HOME/jre/lib/amd64:$JAVA_HOME/jre/lib/amd64/server
# Install R packages (removed hbase)
RUN Rscript -e "install.packages(c(\"rjson\",\"bit64\",\"gridExtra\",\"bit\",\"functional\",\"RJSONIO\",\"R.methodsS3\",\"reshape2\",\"datadr\",\"trelliscope\",\"DBI\",\"RPostgreSQL\",\"RJDBC\",\"glmnet\",  \"testthat\", \"roxygen2\", \"XML\", \"housingData\", \"flexdashboard\"))"
RUN R -e "install.packages('dbplyr', repos = 'http://cran.rstudio.com')"
RUN R -e "install.packages('sparklyr', repos = 'http://cran.rstudio.com')"
# Copy repository packages to filesystem
ADD rhdfs.tar.gz ravro.tar.gz rmr.tar.gz plyrmr.tar.gz rjava.tar.gz /tmp/pkgs/
# Install repository packages
RUN cd /tmp/pkgs && R CMD INSTALL rJava ravro rmr2 plyrmr rhdfs

ADD protobuf-2.5.0.tar.gz Rhipe_0.75.2_hadoop-2.tar.gz /tmp/
RUN cd /tmp/protobuf-2.5.0 && ./configure --prefix=/usr && make && make install && cd .. && rm -rf protobuf-*
RUN cd /tmp/Rhipe && R CMD INSTALL . && cd .. && rm -rf Rhipe

# RUN R CMD "devtools::install_github("rstudio/sparklyr")"
# RUN R CMD options(repos = c(CRAN = "http://cran.rstudio.com"))

# RUN Rscript -e "devtools::install_github("tidyverse/dbplyr")"
# RUN Rscript -e "install.packages(c(\"dplyr\"))"
# RUN Rscript -e "install.packages(c(\"rjson\",\"bit64\",\"bit\",\"functional\",
# \"RJSONIO\",\"R.methodsS3\",\"reshape2\",\"datadr\",\"trelliscope\",\"DBI\",
# \"RPostgreSQL\",\"RJDBC\",\"testthat\",\"roxygen2\",\"XML\",\"housingData\",
# \"flexdashboard\"))"
# RUN R -e "install.packages('rsparkling', repos = 'http://cran.rstudio.com')"
# RUN R -e "install.packages('h2o', repos = 'http://cran.rstudio.com')"
# RUN R -e "install.packages('tensorflow', repos = 'http://cran.rstudio.com')"

# ADD sparklyr-master.zip /tmp/
# RUN mkdir /tmp/sparklyr
# RUN unzip -d /tmp/sparklyr /tmp/sparklyr-master.zip
# RUN cd /tmp/sparklyr/ && R CMD build sparklyr-master
# RUN cd /tmp/sparklyr && R CMD INSTALL sparklyr-master

ADD postgresql-42.1.4.jar /opt

####################
# ENVIRONMENT CONFIG
####################
# Add necessary mods to Renviron file
ADD Renviron /usr/local/lib/R/etc/
ADD hdfs-site.xml /opt/hadoop/etc/hadoop/
ADD core-site.xml /opt/hadoop/etc/hadoop/
ADD log4j.properties /opt/hadoop/etc/hadoop/
# Create symlink to actual Rscript
RUN ln -s /usr/local/bin/Rscript /usr/bin/Rscript
ENV HADOOP_HOME /opt/hadoop
ENV HADOOP_BIN /opt/hadoop/bin
ENV HADOOP_CONF_DIR /opt/hadoop/etc/hadoop

# RUN echo "PATH='/opt/hadoop/bin:/opt/spark/bin:/opt/hive/bin:$PATH'" >> /etc/profile
ENV PATH /opt/hadoop/bin:/opt/spark/bin:/opt/hive/bin::$PATH


# Install Python3
# NOTE: python 3 uses the command 'python3'
RUN easy_install pip
RUN apt-get install -y python3
RUN pip install virtualenv

RUN R -e "install.packages(\"tensorflow\")"

EXPOSE 8787
# VOLUME /home/rstudio

CMD ["/init"]


	
